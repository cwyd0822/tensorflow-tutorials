<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>详解Batchnorm | TensorFlow从入门到精通</title>
    <meta name="description" content="TensorFlow知识点串讲和项目实战">
    
    
    <link rel="preload" href="/assets/css/0.styles.17f32d1e.css" as="style"><link rel="preload" href="/assets/js/app.7619c6b3.js" as="script"><link rel="preload" href="/assets/js/2.e197e3e4.js" as="script"><link rel="preload" href="/assets/js/7.101924b7.js" as="script"><link rel="prefetch" href="/assets/js/10.e19b3701.js"><link rel="prefetch" href="/assets/js/11.bcb96f38.js"><link rel="prefetch" href="/assets/js/3.5ecd6063.js"><link rel="prefetch" href="/assets/js/4.3ff2fe10.js"><link rel="prefetch" href="/assets/js/5.9c8e0335.js"><link rel="prefetch" href="/assets/js/6.612b0a42.js"><link rel="prefetch" href="/assets/js/8.88e1533d.js"><link rel="prefetch" href="/assets/js/9.d0d641fa.js">
    <link rel="stylesheet" href="/assets/css/0.styles.17f32d1e.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" class="home-link router-link-active"><!----> <span class="site-name">TensorFlow从入门到精通</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/" class="nav-link">首页</a></div><div class="nav-item"><a href="/guide/" class="nav-link">目录</a></div><div class="nav-item"><a href="https://github.com/cwyd0822/tensorflow-tutorials" target="_blank" rel="noopener noreferrer" class="nav-link external">
  GitHub
  <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></div><div class="nav-item"><a href="https://blog.csdn.net/keyandi" target="_blank" rel="noopener noreferrer" class="nav-link external">
  CSDN
  <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="/" class="nav-link">首页</a></div><div class="nav-item"><a href="/guide/" class="nav-link">目录</a></div><div class="nav-item"><a href="https://github.com/cwyd0822/tensorflow-tutorials" target="_blank" rel="noopener noreferrer" class="nav-link external">
  GitHub
  <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></div><div class="nav-item"><a href="https://blog.csdn.net/keyandi" target="_blank" rel="noopener noreferrer" class="nav-link external">
  CSDN
  <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></div> <!----></nav>  <ul class="sidebar-links"><li><a href="/guide/" class="sidebar-link">目录</a></li><li><a href="/article/batch-norm.html" class="active sidebar-link">batch-norm函数</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/article/batch-norm.html#为什么batchnorm" class="sidebar-link">为什么Batchnorm</a></li><li class="sidebar-sub-header"><a href="/article/batch-norm.html#batchnorm的原理" class="sidebar-link">Batchnorm的原理</a></li><li class="sidebar-sub-header"><a href="/article/batch-norm.html#batch-normalization的带来的优势" class="sidebar-link">Batch Normalization的带来的优势</a></li><li class="sidebar-sub-header"><a href="/article/batch-norm.html#参考文章：" class="sidebar-link">参考文章：</a></li></ul></li><li><a href="/article/input-producer.html" class="sidebar-link">input-producer函数</a></li><li><a href="/article/batch-and-shuffle-batch.html" class="sidebar-link">batch函数</a></li></ul> </aside> <main class="page"> <div class="theme-default-content content__default"><h1 id="详解batchnorm"><a href="#详解batchnorm" class="header-anchor">#</a> 详解Batchnorm</h1> <blockquote><p>Batchnorm是深度网络中经常用到的加速神经网络训练，加速收敛速度及稳定性的算法，可以说是目前深度网络必不可少的一部分。</p></blockquote> <h2 id="为什么batchnorm"><a href="#为什么batchnorm" class="header-anchor">#</a> 为什么Batchnorm</h2> <ul><li>机器学习领域有个很重要的假设：IID独立同分布假设，就是假设训练数据和测试数据是满足相同分布的，这是通过训练数据获得的模型能够在测试集获得好的效果的一个基本保障。那BatchNorm的作用是什么呢？BatchNorm就是在深度神经网络训练过程中使得每一层神经网络的输入保持相同分布的。</li> <li>深度学习的话尤其是在CV上都需要对数据做归一化，因为深度神经网络主要就是为了学习训练数据的分布，并在测试集上达到很好的泛化效果，但是，如果我们每一个batch输入的数据都具有不同的分布，显然会给网络的训练带来困难。另一方面，数据经过一层层网络计算后，其数据分布也在发生着变化，此现象称为Internal Covariate Shift，接下来会详细解释，会给下一层的网络学习带来困难。batchnorm直译过来就是批规范化，就是为了解决这个分布变化问题。</li> <li>Internal Covariate Shift ：此术语是google小组在论文Batch Normalizatoin 中提出来的，其主要描述的是：训练深度网络的时候经常发生训练困难的问题，因为，每一次参数迭代更新后，上一层网络的输出数据经过这一层网络计算后，数据的分布会发生变化，为下一层网络的学习带来困难（神经网络本来就是要学习数据的分布，要是分布一直在变，学习就很难了），此现象称之为Internal Covariate Shift。之前的解决方案就是使用较小的学习率，和小心的初始化参数，对数据做白化处理，但是显然治标不治本。</li> <li>covariate shift：Internal Covariate Shift 和Covariate Shift具有相似性，但并不是一个东西，前者发生在神经网络的内部，所以是Internal，后者发生在输入数据上。Covariate Shift主要描述的是由于训练数据和测试数据存在分布的差异性，给网络的泛化性和训练速度带来了影响，我们经常使用的方法是做归一化或者白化。想要直观感受的话，看下图：
<img src="https://imgconvert.csdnimg.cn/aHR0cDovL2FpY2hlbndlaS5vc3MtYXAtc291dGhlYXN0LTEuYWxpeXVuY3MuY29tL2dpdGh1Yi9jaWZhcjEwLWNsYXNzaWZpY2F0aW9uLXRlbnNvcmZsb3ctc2xpbS8yLnBuZw?x-oss-process=image/format,png" alt="image"></li></ul> <blockquote><p>举个简单线性分类栗子，假设我们的数据分布如a所示，参数初始化一般是0均值，和较小的方差，此时拟合的y=wx+b
如b图中的橘色线，经过多次迭代后，达到紫色线，此时具有很好的分类效果，但是如果我们将其归一化到0点附近，显然会加快训练速度，如此我们更进一步的通过变换拉大数据之间的相对差异性，那么就更容易区分了。</p></blockquote> <ul><li>Covariate Shift 就是描述的输入数据分布不一致的现象，对数据做归一化当然可以加快训练速度，能对数据做去相关性，突出它们之间的分布相对差异就更好了。Batchnorm做到了，前文已说过，Batchnorm是归一化的一种手段，极限来说，这种方式会减小图像之间的绝对差异，突出相对差异，加快训练速度。所以说，并不是在深度学习的所有领域都可以使用BatchNorm。</li></ul> <h2 id="batchnorm的原理"><a href="#batchnorm的原理" class="header-anchor">#</a> Batchnorm的原理</h2> <p>为了降低Internal Covariate Shift带来的影响，其实只要进行归一化就可以的。比如，我们把network每一层的输出都整为方差为1，均值为0的正态分布，这样看起来是可以解决问题，但是想想，network好不容易学习到的数据特征，被你这样一弄又回到了解放前了，相当于没有学习了。所以这样是不行的，大神想到了一个大招：变换重构，引入了两个可以学习的参数γ、β，当然，这也是算法的灵魂所在：
<img src="https://img-blog.csdn.net/20180417214133986?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3JlbWFuZW50ZWQ=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="image">
具体的算法流程如下:
<img src="https://img-blog.csdn.net/20180417214248222?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3JlbWFuZW50ZWQ=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="">
Batch Normalization 是对一个batch来进行normalization的，例如我们的输入的一个batch为：β=x_(1...m)，输出为：y_i=BN(x)。具体的完整流程如下：</p> <ol><li><p>求出该batch数据x的均值
<img src="https://img-blog.csdn.net/20180417214812576?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3JlbWFuZW50ZWQ=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt=""></p></li> <li><p>求出该batch数据的方差
<img src="https://img-blog.csdn.net/20180417214842955?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3JlbWFuZW50ZWQ=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt=""></p></li> <li><p>对输入数据x做归一化处理，得到：
<img src="https://img-blog.csdn.net/20180417214947300?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3JlbWFuZW50ZWQ=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt=""></p></li> <li><p>最后加入可训练的两个参数：缩放变量γ和平移变量β，计算归一化后的值：
<img src="https://img-blog.csdn.net/20180417215236561?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3JlbWFuZW50ZWQ=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="">
加入了这两个参数之后，网络就可以更加容易的学习到更多的东西了。先想想极端的情况，当缩放变量γ和平移变量β分别等于batch数据的方差和均值时，最后得到的yi就和原来的xi一模一样了，相当于batch normalization没有起作用了。这样就保证了每一次数据经过归一化后还保留的有学习来的特征，同时又能完成归一化这个操作，加速训练。</p> <p>引入参数的更新过程，也就是链式法则：
<img src="https://img-blog.csdn.net/20180417220023946?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3JlbWFuZW50ZWQ=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt=""></p></li></ol> <h3 id="一个简单的例子："><a href="#一个简单的例子：" class="header-anchor">#</a> 一个简单的例子：</h3> <div class="language-python extra-class"><pre class="language-python"><code><span class="token keyword">def</span> <span class="token function">Batchnorm_simple_for_train</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> gamma<span class="token punctuation">,</span>beta<span class="token punctuation">,</span> bn_param<span class="token punctuation">)</span><span class="token punctuation">:</span>
<span class="token triple-quoted-string string">&quot;&quot;&quot;
param:x   : 输入数据，设shape(B,L)
param:gama : 缩放因子  γ
param:beta : 平移因子  β
param:bn_param   : batchnorm所需要的一些参数
   eps      : 接近0的数，防止分母出现0
   momentum : 动量参数，一般为0.9，0.99， 0.999
   running_mean ：滑动平均的方式计算新的均值，训练时计算，为测试数据做准备
   running_var  : 滑动平均的方式计算新的方差，训练时计算，为测试数据做准备
&quot;&quot;&quot;</span>
   running_mean <span class="token operator">=</span> bn_param<span class="token punctuation">[</span><span class="token string">'running_mean'</span><span class="token punctuation">]</span> <span class="token comment">#shape = [B]</span>
   running_var <span class="token operator">=</span> bn_param<span class="token punctuation">[</span><span class="token string">'running_var'</span><span class="token punctuation">]</span>   <span class="token comment">#shape = [B]</span>
   results <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">.</span> <span class="token comment"># 建立一个新的变量</span>
   x_mean<span class="token operator">=</span>x<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>  <span class="token comment"># 计算x的均值</span>
   x_var<span class="token operator">=</span>x<span class="token punctuation">.</span>var<span class="token punctuation">(</span>axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>    <span class="token comment"># 计算方差</span>
   x_normalized<span class="token operator">=</span><span class="token punctuation">(</span>x<span class="token operator">-</span>x_mean<span class="token punctuation">)</span><span class="token operator">/</span>np<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>x_var<span class="token operator">+</span>eps<span class="token punctuation">)</span>       <span class="token comment"># 归一化</span>
   results <span class="token operator">=</span> gamma <span class="token operator">*</span> x_normalized <span class="token operator">+</span> beta            <span class="token comment"># 缩放平移</span>
   running_mean <span class="token operator">=</span> momentum <span class="token operator">*</span> running_mean <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> momentum<span class="token punctuation">)</span> <span class="token operator">*</span> x_mean
   running_var <span class="token operator">=</span> momentum <span class="token operator">*</span> running_var <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> momentum<span class="token punctuation">)</span> <span class="token operator">*</span> x_var    <span class="token comment">#记录新的值</span>
   bn_param<span class="token punctuation">[</span><span class="token string">'running_mean'</span><span class="token punctuation">]</span> <span class="token operator">=</span> running_mean
   bn_param<span class="token punctuation">[</span><span class="token string">'running_var'</span><span class="token punctuation">]</span> <span class="token operator">=</span> running_var   
   <span class="token keyword">return</span> results <span class="token punctuation">,</span> bn_param
</code></pre></div><p>看完这个代码是不是对batchnorm有了一个清晰的理解，首先计算均值和方差，然后归一化，然后缩放和平移，完事！但是这是在训练中完成的任务，每次训练给一个批量，然后计算批量的均值方差，但是在测试的时候可不是这样，测试的时候每次只输入一张图片，这怎么计算批量的均值和方差，于是，就有了代码中下面两行，在训练的时候实现计算好mean var测试的时候直接拿来用就可以了，不用计算均值和方差。</p> <div class="language-python extra-class"><pre class="language-python"><code>running_mean <span class="token operator">=</span> momentum <span class="token operator">*</span> running_mean <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token operator">-</span> momentum<span class="token punctuation">)</span> <span class="token operator">*</span> x_mean
running_var <span class="token operator">=</span> momentum <span class="token operator">*</span> running_var <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span>momentum<span class="token punctuation">)</span> <span class="token operator">*</span> x_var
</code></pre></div><p>所以，测试的时候是这样的：</p> <div class="language-python extra-class"><pre class="language-python"><code><span class="token keyword">def</span> <span class="token function">Batchnorm_simple_for_test</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> gamma<span class="token punctuation">,</span>beta<span class="token punctuation">,</span> bn_param<span class="token punctuation">)</span><span class="token punctuation">:</span>
<span class="token triple-quoted-string string">&quot;&quot;&quot;
param:x   : 输入数据，设shape(B,L)
param:gama : 缩放因子  γ
param:beta : 平移因子  β
param:bn_param   : batchnorm所需要的一些参数
   eps      : 接近0的数，防止分母出现0
   momentum : 动量参数，一般为0.9，0.99， 0.999
   running_mean ：滑动平均的方式计算新的均值，训练时计算，为测试数据做准备
   running_var  : 滑动平均的方式计算新的方差，训练时计算，为测试数据做准备
&quot;&quot;&quot;</span>
   running_mean <span class="token operator">=</span> bn_param<span class="token punctuation">[</span><span class="token string">'running_mean'</span><span class="token punctuation">]</span> <span class="token comment">#shape = [B]</span>
   running_var <span class="token operator">=</span> bn_param<span class="token punctuation">[</span><span class="token string">'running_var'</span><span class="token punctuation">]</span>   <span class="token comment">#shape = [B]</span>
   results <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">.</span> <span class="token comment"># 建立一个新的变量</span>
   x_normalized<span class="token operator">=</span><span class="token punctuation">(</span>x<span class="token operator">-</span>running_mean <span class="token punctuation">)</span><span class="token operator">/</span>np<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>running_var <span class="token operator">+</span>eps<span class="token punctuation">)</span>       <span class="token comment"># 归一化</span>
   results <span class="token operator">=</span> gamma <span class="token operator">*</span> x_normalized <span class="token operator">+</span> beta            <span class="token comment"># 缩放平移</span>
   <span class="token keyword">return</span> results <span class="token punctuation">,</span> bn_param
</code></pre></div><h2 id="batch-normalization的带来的优势"><a href="#batch-normalization的带来的优势" class="header-anchor">#</a> Batch Normalization的带来的优势</h2> <ul><li><p>没有它之前，需要小心的调整学习率和权重初始化，但是有了BN可以放心的使用大学习率，但是使用了BN，就不用小心的调参了，较大的学习率极大的提高了学习速度，</p></li> <li><p>Batchnorm本身上也是一种正则的方式，可以代替其他正则方式如dropout等</p></li> <li><p>另外，个人认为，batchnorm降低了数据之间的绝对差异，有一个去相关的性质，更多的考虑相对差异性，因此在分类任务上具有更好的效果</p></li></ul> <h2 id="参考文章："><a href="#参考文章：" class="header-anchor">#</a> 参考文章：</h2> <ul><li><a href="https://blog.csdn.net/remanented/article/details/79980486" target="_blank" rel="noopener noreferrer">《Batch Normalization 和 Group Normalization》<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></li></ul></div> <footer class="page-edit"><!----> <!----></footer> <div class="page-nav"><p class="inner"><span class="prev">
      ←
      <a href="/guide/" class="prev">目录</a></span> <span class="next"><a href="/article/input-producer.html">input-producer函数</a>
      →
    </span></p></div> </main></div><div class="global-ui"></div></div>
    <script src="/assets/js/app.7619c6b3.js" defer></script><script src="/assets/js/2.e197e3e4.js" defer></script><script src="/assets/js/7.101924b7.js" defer></script>
  </body>
</html>
